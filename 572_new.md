#572 Revise

[toc]

##Page Rank

### Algorithm

> We assume page A has pages T1...Tn which point to it (i.e., are citations). Theparameter d is a damping factor which can be set between 0 and 1. We usually setd to 0.85. There are more details about d in the next section. Also C(A) is definedas the number of links going out of page A.

$$ PR(A) = (1-d) + d (PR(T1)/C(T1) + ... + PR(Tn)/C(Tn)) $$

That the PageRanks form a probability distribution over web pages, so the sum ofall web pages' PageRanks will be one.

* PR(A) is PageRank of Page A (one we want to calculate)* PR(T1) is the PageRank of Site T1 pointing to page A* C(T1) is the number of outgoing links of page T1* PR(Tn)/C(Tn) : If page A has a backlink from page “Tn” the share of thevote page A will get is “PR(Tn)/C(Tn)”* d(…) : All these fractions of votes are added together but, to stop theother pages having too much influence, this total vote is “dampeddown” by multiplying it by the factor “d”, (0.85)* (1-d) : Since “sum of all web pages' PageRanks will be one”. Even if thed(…) is 0 then the page will get a small PR of 0.15

1. If PR(A) represents the PageRank of node A in directed graph G and C(A) is the number of outgoing links of A, define the PageRank formula

$$ PR(A) = (1-d) + d (PR(T1)/C(T1) + ... + PR(Tn)/C(Tn)) $$

2. Make sure you know how to compute the PageRank for simple graphs like the ones below.

$$ PR(A) = (1 - d) + d( PR(C) / C(C) ) $$
$$ PR(B) = (1 - d) + d( PR(A) / C(A) ) $$
$$ PR(C) = (1 - d) + d( PR(A) / C(A) + PR(B) / C(B) + PR(D) / C(D) ) $$
$$ PR(D) = (1 - d) = 0.15 $$


## HITS

1. What are the names given to the two types of nodes determined byHITS?

	* **Authorities** are pages that are recognized as providingsignificant, trustworthy, and useful information on atopic.
	
	* **Hubs** are index pages that provide lots of useful linksto relevant content pages (topic authorities).


2. Is the HITS algorithm carried out once the query is entered or afterthe query results have been determined?
	* after
3. What sort of graph is produced by HITS?

	* For a specific query Q, let the set of documents returned by astandard search engine be called the root set R.	* Initialize S to R.	* Add to S all pages pointed to by any page in R.	* Add to S all pages that point to any page in R.
	* **bipartite graph**
4. How does the HITS algorithm compute the Authority score?
	* Update each node's Authority score to be equal to the sum ofthe Hub Scores of each node that points to it. Normalize the values by dividing each Authority score by the sum of the squares of all Authority scores.5. How does the HITS algorithm compute the Hub score?
	* Update each node's Hub Score to be equal to the sum of the AuthorityScores of each node that it points to. Normalize the values by dividing each Hub score by the sum of the squares of all Hub scores.


## Search Engine Advertising

1. What is the difference between Google’s AdWords and AdSense programs?
	 * Google’s system of paid ads on the search engine is called AdWords. Auction for each keyword and highest bidder may come first but not guarantee.
	 * Google’s AdSense program offers to place ads on third-party websites, where the ads are relevant to the site’s content; it includes text and image ads;2. Approximately how much money did Google take in on advertisinglast year: 500 million, 1 billion, 10 billion, 50 billion, 100 billion?

	* 100 billion
3. What does SEO stand for?

 * Search Engine Optimization (SEO)
4. Does Google accept banner ads?

	* No5. Describe the auction process Google uses to determine the placement of ads

	* For every keyword phrase there is an auction where bidders agree to pay a certain amount to Google if their ad is clicked on; 
	* Highest bidder may come first, but not always as Google uses CTR*Bid amount among other factors to determine placement	* Click Through Rate
6. Describe the four possible forms of keyword matching that Google offers
	看卷子
	1. Broad Match: any order	2. Exact Match: 	3. Phrase Match	4. Negative Keyword

## Map/Reduce Questions

1. What is Hadoop?

	看卷子
 * Apache Hadoop is an open-source implementation of map/reduce written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware.
 * handle common occured hardware failure and the storage is HDFS
2. Name two companies offering cloud services.
	
	* Amazon Web Services (Amazon)
	* Windows Azure (Microsoft)
	* Google Could Platform (Google)
3. For the word occurrences problem describe what the map phase does
	
	0. Input the document content with type of Text
	1. Split the words in the documents into single words
	2. For each word make a key-vaue pair where the key is the word with type Text and value is 1 with the type IntWritable
	3. Output the key-value pairs4. For the word occurrences problem describe what the reduce phase does
	
	1. Input the word as key and its corresponding values with the type of Iterable<IntWritable>.
	2. For each word key, sum its associated list of values
	3. Output word as key and sum as value
5. Describe how Map/Reduce handles failure of a node

	1. Worst case: the compute node where the Master is executing fails		* Result: the entire map-reduce job must be restarted 
	2. The compute node of a Map worker fails
		* This is detected by the Master and all Map tasks that were assigned are re-done		* The Master sets the status of each Map task to idle and re-schedules them when a worker becomes available		* The Master informs each Reduce task of the location of its new input 
	3. The compute node of a Reduce worker fails
		* The Master sets the status of its currently executing Reduce tasks to idle and they will be re-scheduled on another reduce worker later

## Knowledge Systems Questions

1. Describe the difference between a taxonomy and an ontology

	a shorter answer: 
	
	**taxonomy** is usually only a hierarchy of concepts (i.e. the only relation between the concepts is parent/child, or subClass/superClass, or broader/narrower) 
	
	in an **ontology**, arbitrary complex relations between concepts can be expressed too (X marriedTo Y; or A worksFor B; or C locatedIn D, etc )
2. Describe three types of knowledge typically held by KnowledgeBases
	
	* taxonomic knowledge
	* factual knowledge
	* temporal knowledge
	* emerging knowledge	* terminological knowledge
3. Is Wikipedia an ontology?

	### No
4. Is WordNet a KnowledgeBase?

	Yes
5. WordNet uses the terms synset, hyponym, meronym and holonym; define them

	synset: 
	
	* **hypernym**: Epxressive of an IS-A relationship between nouns.
	* **hyponym**: opposite of hypernym
	* **meronym**: Expresses a PART-OF relationships between nouns. 
	* **holonym**: opposite of meronym
	6. The WikiMedia Foundation sponsors Wikipedia and WikiData; what is WikiData and how does it relate to Wikipedia?

* Wikidata aims to create a free rdf-like KB about the world that can be read/edited by humans & machines
* Increase the quality and lower the maintenance costs of Wikipedia
	* WikiData replaces a lot of manual or bot effort
	* Make Wikipedia more attractive by including more data and visualization
	* Enable automatic creation of millions of stubs in more than 100 languages 

## Query Processing Questions

1. Five strategies were mentioned for speeding up the answering of queries; describe each one in two or three sentences

	1. **High-idf Query Terms Onl**y: ignore the word that contributes little, only accumulate the scores from the words with high-idf
	2. **Docs Containing Many Query Terms**: For multi-term queries, only compute scores for docs containing several of the query terms
	3. **Champion Lists Heuristic**: Pre-compute for each dictionary term t, the r docs of highest weight (tf-idf) in t’s postings; Call this the champion list for t. At query time, only compute scores for docs in thechampion list of some query term
	4. **Modeling Authority**: Assign to each document a query-independent quality score in [0,1] to each document d, Thus, a quantity like the number of citations is scaled into [0,1]
	
### 5. **Reorganize the Inverted List**: ?
2. Does Google get more queries from desktop computers or from mobile devices?

	No, There are now more mobile queries than desktop
3. Does Google use human testers to evaluate search results?

	Yes, 

	* Show real people experimental search results	* Ask how good the results are	* Ratings aggregated across raters	* Published guidelines explain criteria for raters	* Tools support doing this in an automated way

## Homework #4

1. Does Lucene come bundled with Solr?

	* Yes2. What sort of inverted index is produced by Lucene?
	
	* Positional Inverted Index3. What is the purpose of the Lucene tokenizers?
	
	**Split documents into tokens (words)**
	
	* Splits words at punctuation characters, removing punctuation. However, a dot that's not followed by whitespace is considered part of a token.
	* Splits words at hyphens, unless there's a number in the token, in which case the whole token is interpreted as a product number and is not split.
	* Recognizes email addresses and internet hostnames as one token.
4. Solr uses two types of scoring to produce search results, what are they?

	* Lucene scores using a combination of **TF-IDF** and **vector closeness** (cosine similarity)
5. What port does Solr run on?
	
	* 8983
6. Define a database shard

	* A **database** **shard** is a horizontal partition of data in a **database** or search engine. Each **shard** is held on a separate **database** server instance, to spread the load.
7. What language is Solr written in?

	* Java

## Spell Checking and Correction

1. Name three types of spelling errors

	1. Non-word errors		* graffe -> giraffe	2. Typographical errors (flip the e and r)		* three -> there (especially bad as both are words)	3. Cognitive errors (homophones, sounds alike)		* piece -> peace,		* too -> two		* your -> you’re
2. There are two approaches for correcting a word that is not in the dictionary, what are they?

	* Shortest weighted edit distance
	* Highest noisy channel probability
3. How are n-grams used to solve the spelling correction problem?

	* An **n-gram** model is a type of probabilistic language model forpredicting the next item in a sequence	* Two benefits of **n-gram** models (and algorithms that use them)are simplicity and scalability – with larger n, a model can storemore context with a well-understood space–time tradeoff,enabling small experiments to scale up efficiently
4. Define edit distance

	* **Edit distance** is a way of quantifying how dissimilar two strings (e.g. words) are to one another by counting the minimum number of operations required to transform one string into the other
5. What is the name of the dictionary used in Norvig’s spelling corrector program?
	
	* big.txt
6. Does Norvig’s spelling corrector program only use corrections of edit distance 1 or does it use both edit distance 1 and 2?

	* use distance 1 and 2
7. What are the computing time and space requirements for the Levenshtein algorithm?

	* Time: O(nm)	* Space: O(nm)	* Backtrace: O(n + m)

	
## Snippets

1. Are snippets always taken from the body of a web page?

	Nope
2. Does the location of the sentence that contains the query keywords matter when it comes to choose which sentence to display as a snippet?

	* Yes, The snippet may be generated based on the type of query or the location of the query terms in the document.
3. Does Google use meta information in a web page to determine a snippet?

	* Yes, Google uses the meta description (if there is one) as the default for asnippet
4. Define ”rich snippets”

	* **Rich Snippets**: a mechanism for website developers to include information that Google's results algorithm will display
5. Where can you find the specification for rich snippets?

	* `schema.org` They establish the website schema.org which defines the mechanism for creating rich snippets, They decide to standardize on microdata format
6. Is the schema.org formalism an object hierarchy?

	* Yes, **Schema.org** defines an object hierarchy

	
## Clustering

1. Name a search engine other than Google/Bing/Yahoo that emphasizes clustering of results

	* **yippy.com**
2. Describe the difference between hard clustering and soft clustering

	* **Hard clustering**: Each document belongs to exactly one cluster
	* **Soft clustering**: A document can belong to more than one cluster.
3. What definition of similarity is used by the k-means clustering algorithm?

	* **Clustering algorithm's** 'Closeness' is measured by **cosine similarity**, a variation of Euclidean distance 
	* **Cosine similarity** is a measure of similarity between two vectors of an inner product space that measures the cosine of the angle between them.
4. Describe the **optimal k-means optimization problem**; is it NP-Hard?
	* The **optimal k-means problem** calls for finding cluster centers that minimize the intra-class variance, i.e. the sum of squared distances from each data point being clustered to its cluster center
	* Finding an exact solution to the k-means problem for arbitrary input is **NP-hard**
5. How are the k means typically chosen in the k-means algorithm?

	1. Select K points as initial centroids	2. repeat		* form K clusters by assigning each point to its closest centroid		* re-compute the centroid of each cluster	3. until centroids do not change		* the algorithm will always terminate, however it does not always find the optimal solution		* this is an example of a greedy algorithm
6. What is the **computing time** for the k-means algorithm?

	* Computing distance between two docs is O(M) where M is the dimensionality of the vectors	* Re-assigning clusters: O(KN) distance computations, or O(KNM)	* Computing centroids: Each doc gets added once to some centroid: O(NM)	* Assume these two steps are each done once for I iterations: O(IKNM)	* Note:		* M is the size of the vector		* N is the number of vectors (items)		* K is the number of clusters		* I number of iterations, depends upon convergence
7. Describe the **agglomerative clustering algorithm**
	1. Compute the distance matrix between the input data points	2. Let each data point be a cluster	3. Repeat		* Merge the two closest clusters		* Update the distance matrix	4. Until only a single cluster remai
8. What is a dendrogram?

	* A dendrogram is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering

	
## Question Answering

1. Name the four “W” questions used by question/answering systems
	
	* Who
	* When
	* Where
	* What
2. Do question answering systems use search engine results to help answer questions?

	* Yes, use search engine to retrive the related docs
3. What are the TREC conferences?
	* **Text Retrieval Conferences (TREC)**: An annual information retrieval conference and competition, the purpose of which is to support and further research within the information retrieval community.
4. There are five general approaches to question answering; describe each one

	* **Part-of-Speech Tagging**: a piece of software that reads text in some language and assigns parts of speech to each word, such as noun, verb, adjective, etc.	* **Parsing**: analyzing a sentence into its parts and capturing their syntactic roles	* **Named Entity Extraction**: a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons,organizations, locations, expressions of times, quantities ...	* **Determining Semantic Relations**: semantic relations are concepts or meanings between entities. [School] is a kind of [educational institution]	* **Dictionaries**
5. Define the Mean Reciprocal Rank

	* The **mean reciprocal rank** is the average of the reciprocal ranks of results for a sample of queries. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer.

	$$ MRR = {1 \over |Q|} \sum_{i = 1}^{|Q|} {1 \over rank_i} $$
	
## Classification

1. How does classification differ from clustering?

	* In general, in **classification** you have a set of predefined classes and want to know which class a new object belongs to.
	* **Clustering** tries to group a set of objects and find whether there is some relationship between the objects.
2. Define a centroid of a set of n points, each point a vector of length m

	$$ \mu(C) = {1 \over n} \sum_{d \in C} d$$
	
	$d$ is the doc vector, $C$ is the set of documents $n$ is the number of points 
3. The Rocchio algorithm has two phases, what are they?

	1. Training
	2. Testing
4. In the Rocchio algorithm how are the centroids determined?

	### Determine the centroid u(c) that the point is closest to and then assign it to c
	
	###Time Complexity
5. Describe the k-nearest neighbor algorithm
	1. Initially we assume we have a set of N documents that have already been classified
	2. To classify a document d
		1. locate among the N documents the k closest ones
		2. from these k neighbors, pick the class that occurs mostoften, the majority class, and use that as the label for d
 6. What is the contiguity hypothesis?
	
	* if documents do form contiguous regions in space, then kNN makes sense
7. How is k chosen in the k-nearest neighbor algorithm?
	* The best choice of k depends upon the data 
		* larger reduce noise, but make the boundries less distince
	* k is odd can help to avoid tied votes in bianry classification8. What is a Voroni diagram?
	* A Voronoi diagram is a partitioning of a plane into regions based on distance to points in a specific subset of the plane
	* For each class label there is a corresponding region consisting of all points closer to that class label than to any other. These regions are called Voronoi cells